# faris-engineering-journal
SDC journal

---

## February 4th, 2019
**1. Choosing FEC Project and Service**
- The chosen project for our SDC group is the ESPN NFL Team Page. I'll be working on the standings service that was originally made by Kevin Phung. This service renders the NFL Team standings within the different divisions of the league. There are two components to this service that are rendered to the page:

  - **Full Standings Component**: This component will render all of the NFL team standings within each sub division of the league. This component isn't rendered onto the main page but is accessed through the standings component. Sub divisions are hard coded, but the information from the database that is rendered to this page include:

    - Team name
    - Team logo
    - Wins
    - Losses
    - Ties
    - Percentage
    - Points for
    - Points against
    - Difference

  ![full standings][one]

  - **Standings Component**: This component renders one sub division of the Full Standings Component to the main page, which is always the NFC WEST division. The information from the database that gets rendered is the same as the Full Standings Component.

  ![standings][two]

**2. Choosing SQL DBMS**
- For my SQL DBMS choice I've chosen to implement a PostgreSQL database. I archived all of Kevin's MongoDB files with an underscore extension in case I'll need it again. Implementing the PostgreSQL was not difficult due to my previous knowledge in implementing one during the FEC project for my service. To utilize PostgresSQL I installed it with homebrew and used the PG client paired with KnexJS and Bookshelf to implement the database easily. I tested an insertion of the same mock data that Kevin created with my new database and it worked successfully. The next step will be to create a script that can insert 10 million records.

**3. Generating 10,000,000 records**
- I've completed the seed script that will execute the records to my SQL database. In summary I created a function that generates the data using a for loop. The only parameter supplied is how many records are to be generated. For quick record generation I used faker, but I'll need to edit the types of images that are supplied for the team logo because faker doesn't have a good method to supply team logo images. What I'll probably end up doing is hosting the sports team logos on S3 and randomize the selection so essentially each record looks more like a sport team. The number of wins,losses, and ties generated by faker also seem extreme so I'll probably tweak this later too.

- Generating 1 million records proved to be a success. It took about 2 minutes and 21 seconds to complete. At about 1.5 million records is when I get an allocation failure. I've copied the full error message below for reference.

```
<--- JS stacktrace --->

==== JS stack trace =========================================

Security context: 0xfc4822cf781 <JS Object>
    2: randomWord [/Users/fh/sdc/kevin-services-standings/node_modules/faker/lib/random.js:~116] [pc=0x30e249a6abcb] (this=0x1844f5d85be1 <a Random with map 0x1897ee7ef459>,type=0xfc482204381 <undefined>)
    3: arguments adaptor frame: 0->1
    4: generateDataSet [/Users/fh/sdc/kevin-services-standings/database/seeds/seed.js:~11] [pc=0x30e249a74b29] (this=0x22b8e5698c19 <JS Global Object>,numOf...

FATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - JavaScript heap out of memory
 1: node::Abort() [/Users/fh/.nvm/versions/node/v6.13.1/bin/node]
 2: node::FatalException(v8::Isolate*, v8::Local<v8::Value>, v8::Local<v8::Message>) [/Users/fh/.nvm/versions/node/v6.13.1/bin/node]
 3: v8::internal::V8::FatalProcessOutOfMemory(char const*, bool) [/Users/fh/.nvm/versions/node/v6.13.1/bin/node]
 4: v8::internal::Factory::NewFixedArray(int, v8::internal::PretenureFlag) [/Users/fh/.nvm/versions/node/v6.13.1/bin/node]
 5: v8::internal::TypeFeedbackVector::New(v8::internal::Isolate*, v8::internal::Handle<v8::internal::TypeFeedbackMetadata>) [/Users/fh/.nvm/versions/node/v6.13.1/bin/node]
 6: v8::internal::(anonymous namespace)::Ensurefactor: test 10m records. Fails at 1.5m
reFeedbackVector(v8::internal::CompilationInfo*) [/Users/fh/.nvm/versions/node/v6.13.1/bin/node]
 7: v8::internal::(anonymous namespace)::GenerateBaselineCode(v8::internal::CompilationInfo*) [/Users/fh/.nvm/versions/node/v6.13.1/bin/node]
 8: v8::internal::(anonymous namespace)::GetUnoptimizedCodeCommon(v8::internal::CompilationInfo*) [/Users/fh/.nvm/versions/node/v6.13.1/bin/node]
 9: v8::internal::Compiler::Compile(v8::internal::Handle<v8::internal::JSFunction>, v8::internal::Compiler::ClearExceptionFlag) [/Users/fh/.nvm/versions/node/v6.13.1/bin/node]
10: v8::internal::Runtime_CompileLazy(int, v8::internal::Object**, v8::internal::Isolate*) [/Users/fh/.nvm/versions/node/v6.13.1/bin/node]
11: 0x30e2493092a7
```
- I'll need to perform some fine-tuning with my seed script. One idea I have is turning the seed function into an async function so that records are inserted one at a time, but this would require me to refactor the code that uses knex's bulkInsert method, which I find to be helpful overall. I'll need to play around some more to figure out how I can insert 10 million records in about 50 minutes.

- I successfully completed the insertion of 10m records! The official time is 24min8s56ms. They key was chaining knex's batch insert in promise format.

```
exports.seed = knex => knex('standings').del()
  .then(() => knex.batchInsert('standings', generateDataSet(1000000), 1000)
    .then(() => knex.batchInsert('standings', generateDataSet(1000000), 1000))
    .then(() => knex.batchInsert('standings', generateDataSet(1000000), 1000))
    .then(() => knex.batchInsert('standings', generateDataSet(1000000), 1000))
    .then(() => knex.batchInsert('standings', generateDataSet(1000000), 1000))
    .then(() => knex.batchInsert('standings', generateDataSet(1000000), 1000))
    .then(() => knex.batchInsert('standings', generateDataSet(1000000), 1000))
    .then(() => knex.batchInsert('standings', generateDataSet(1000000), 1000))
    .then(() => knex.batchInsert('standings', generateDataSet(1000000), 1000))
    .then(() => knex.batchInsert('standings', generateDataSet(1000000), 1000)));

```

- I met with my team member Amit and we had discussed how chaining insertions might be the key. This is because it's hard to hold so many records in any one function. I knew that the batch insert utility could insert 1m records in 1000 chunks, so I decided to try chaining ten batch inserts in that same format and success!

- With 10 million records in the database I had to make adjustments to my server code so that the API call would not fetch all 10 million records. To do this I went through Bookshelf's documentation and found that the collection had a couple methods I could utilize. In summary I had to make sure I was performing the query below each time I was making an API request for the data:

```
  SELECT * FROM STANDINGS ORDER BY ID DESC LIMIT 100;
```

- Luckily Bookshelf has the OrderBy and Query methods to help structure the API call on the Standings collections to mimic the query above.

- Work for generating 10 million records is for the most part done. Now I'll be conducting some benchmarking tests on the database before I start with my NoSQL Database.
---

[one]: images/fullstandingscomponentsnapshot.png
[two]: images/standingscomponentsnapshot.png